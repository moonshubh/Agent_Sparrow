# =============================================================================
# Agent Sparrow Model Configuration
# Single source of truth for all LLM model specifications
# =============================================================================
# Provider auto-inference: if omitted, inferred from model_id pattern:
#   - "gemini-*" → google
#   - "grok-*" → xai
#   - "*/*" (contains slash) → openrouter
#   - "models/*" → google (Gemini embeddings)
#   - Otherwise → google (default)
# =============================================================================

# Global quota enforcement configuration.
# Safety margin is applied to all quotas to avoid accidental overage.
# Rate limits in this file are internal budgets, not provider guarantees.
rate_limiting:
  safety_margin: 0.06   # ~6% headroom (configurable)

# -----------------------------------------------------------------------------
# COORDINATOR MODELS - Primary agents that interact with users
# -----------------------------------------------------------------------------
coordinators:
  # Default coordinator for Google provider
  google:
    model_id: "gemini-3-flash-preview"
    temperature: 1.0          # Gemini 3.x recommended
    context_window: 1048576
    rate_limits:
      rpm: 1000
      tpm: 1000000
      rpd: 10000

  # Coordinator quota bucket when subagent orchestration is enabled.
  # NOTE: Model can be identical to `coordinators.google` but limits are separate.
  google_with_subagents:
    model_id: "gemini-3-flash-preview"
    temperature: 1.0
    context_window: 1048576
    rate_limits:
      rpm: 1000
      tpm: 1000000
      rpd: 10000

  # Default coordinator for XAI provider
  xai:
    model_id: "grok-4-1-fast-reasoning"
    temperature: 0.2
    context_window: 2000000
    rate_limits:
      rpm: 60
      rpd: 1000

  xai_with_subagents:
    model_id: "grok-4-1-fast-reasoning"
    temperature: 0.2
    context_window: 2000000
    rate_limits:
      rpm: 45
      rpd: 750

  # Default coordinator for OpenRouter provider
  openrouter:
    model_id: "x-ai/grok-4.1-fast"
    provider: "openrouter"
    temperature: 0.2
    context_window: 2000000
    always_enable_reasoning: true
    rate_limits:
      rpm: 60
      rpd: 1000

  openrouter_with_subagents:
    model_id: "x-ai/grok-4.1-fast"
    provider: "openrouter"
    temperature: 0.2
    context_window: 2000000
    always_enable_reasoning: true
    rate_limits:
      rpm: 45
      rpd: 750

  # Default coordinator for Minimax provider (uses direct Minimax API)
  # Requires MINIMAX_API_KEY to be set. Routes through OpenRouter code path.
  # Minimax M2.1 uses Interleaved Thinking - reasoning_details preserved automatically.
  minimax:
    model_id: "minimax/MiniMax-M2.1"
    provider: "openrouter"        # Uses OpenRouter code path, auto-routed to Minimax API
    temperature: 1.0
    top_p: 0.95
    top_k: 40
    context_window: 204800        # 200K token context window
    rate_limits:
      rpm: 100
      rpd: 5000

  minimax_with_subagents:
    model_id: "minimax/MiniMax-M2.1"
    provider: "openrouter"
    temperature: 1.0
    top_p: 0.95
    top_k: 40
    context_window: 204800        # 200K token context window
    rate_limits:
      rpm: 80
      rpd: 4000

# -----------------------------------------------------------------------------
# INTERNAL MODELS - System components (not user-selectable)
# -----------------------------------------------------------------------------
internal:
  # Summarization / thread-state extraction
  summarizer:
    model_id: "gemini-2.5-flash-preview-09-2025"
    provider: "google"        # Explicit: always Google direct
    temperature: 0.2
    context_window: 1048576
    rate_limits:
      rpm: 1000
      tpm: 1000000
      rpd: 10000

  # Helper model for lightweight rewrite/rerank/summarize (GemmaHelper)
  helper:
    model_id: "gemini-2.5-flash-lite"
    provider: "google"
    temperature: 0.2
    context_window: 1048576
    rate_limits:
      rpm: 4000
      tpm: 4000000
      rpd: 1000000

  # Image generation model (tool-only; not user-selectable)
  image:
    model_id: "gemini-3-pro-image-preview"
    provider: "google"
    temperature: 1.0
    context_window: 1048576
    rate_limits:
      rpm: 20
      tpm: 100000
      rpd: 250

  # Minimax MCP tools (web search + image understanding)
  minimax_tools:
    model_id: "minimax/MiniMax-M2.1"
    provider: "openrouter"
    temperature: 1.0
    top_p: 0.95
    top_k: 40
    context_window: 204800
    rate_limits:
      rpm: 120
      rpd: 5000

  # FeedMe extraction / structured parsing
  feedme:
    model_id: "gemini-2.5-flash-lite"
    provider: "google"
    temperature: 0.3
    context_window: 1048576
    rate_limits:
      rpm: 4000
      tpm: 4000000
      rpd: 1000000

  # Gemini Search Grounding model
  grounding:
    model_id: "gemini-2.5-flash"
    provider: "google"
    temperature: 0.2
    context_window: 1048576
    rate_limits:
      rpm: 1000
      tpm: 1000000
      rpd: 10000

  # Vector embeddings
  embedding:
    model_id: "models/gemini-embedding-001"
    provider: "google"
    context_window: 2048
    embedding_dims: 3072
    rate_limits:
      rpm: 3000
      tpm: 1000000
      rpd: 1000000

# -----------------------------------------------------------------------------
# SUBAGENT MODELS - DeepAgents subagents for specialized tasks
# Each subagent can have its own model, or fallback to default
# -----------------------------------------------------------------------------
subagents:
  # Default model for all subagents - Minimax M2.1
  # Uses Minimax API via OpenAI-compatible wrapper when MINIMAX_API_KEY is set.
  # Minimax M2.1 features:
  # - 200K context window
  # - Interleaved Thinking (reasoning preserved across turns)
  # - Optimized for coding and agentic workflows
  _default:
    model_id: "minimax/MiniMax-M2.1"
    provider: "openrouter"            # Routes to Minimax API when MINIMAX_API_KEY set
    temperature: 1.0
    top_p: 0.95
    top_k: 40
    context_window: 204800            # 200K tokens
    rate_limits:
      rpm: 60
      rpd: 1000

  # IMPORTANT: Keys are the runtime subagent names (match `subagents.py`).
  # All subagents now use Minimax M2.1 via _default
  research-agent:
    # Uses _default (Minimax M2.1)

  log-diagnoser:
    # Uses _default (Minimax M2.1)

  db-retrieval:
    # Uses _default (Minimax M2.1)

  # Explorer subagent - broad discovery
  explorer:
    # Uses _default (Minimax M2.1)

  # Draft writer subagent - structured writing and response composition
  draft-writer:
    # Uses _default (Minimax M2.1)

  # Data analyst subagent - data retrieval and trend analysis
  data-analyst:
    # Uses _default (Minimax M2.1)

# -----------------------------------------------------------------------------
# ZENDESK MODELS - Separate coordinator/subagent config and quotas
# -----------------------------------------------------------------------------
zendesk:
  coordinators:
    google:
      model_id: "gemini-3-flash-preview"
      temperature: 1.0
      context_window: 1048576
      rate_limits:
        rpm: 150
        rpd: 5000

    google_with_subagents:
      model_id: "gemini-3-flash-preview"
      temperature: 1.0
      context_window: 1048576
      rate_limits:
        rpm: 120
        rpd: 4000
  subagents:
    _default:
      model_id: "minimax/MiniMax-M2.1"
      provider: "openrouter"
      temperature: 1.0
      top_p: 0.95
      top_k: 40
      context_window: 204800
      rate_limits:
        rpm: 60
        rpd: 1000

    # Explicit null entries fall back to _default (Minimax M2.1).
    research-agent:
    log-diagnoser:
    db-retrieval:
    explorer:

# -----------------------------------------------------------------------------
# FALLBACK CONFIGURATION
# -----------------------------------------------------------------------------
fallback:
  # When a subagent/internal model fails, fallback to coordinator
  strategy: "coordinator"     # Options: "coordinator", "none"
  coordinator_provider: "google"
